{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger ,ReduceLROnPlateau\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "from Utils import data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05f0bc",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### ðŸ“¦ Architecture Overview\n",
    "\n",
    "The **3D U-Net** follows an encoder-decoder structure with skip connections and is designed to capture both **spatial context** and **fine-grained localization** in volumetric data.\n",
    "\n",
    "#### ðŸ”¹ Encoder Path (Contracting)\n",
    "- Extracts high-level features while reducing spatial resolution.\n",
    "- Each block applies two 3D convolutions followed by batch normalization and ReLU activation.\n",
    "- Downsampling is done using 3D max pooling.\n",
    "\n",
    "#### ðŸ”¹ Bottleneck\n",
    "- The deepest part of the network.\n",
    "- Contains convolutional layers without pooling or upsampling.\n",
    "- Acts as a bridge between encoder and decoder.\n",
    "\n",
    "#### ðŸ”¹ Decoder Path (Expanding)\n",
    "- Gradually restores spatial dimensions using 3D upsampling.\n",
    "- Each upsampling block is followed by concatenation with the corresponding encoder feature map (skip connection), helping retain spatial detail.\n",
    "- Then it applies convolutional layers to refine the merged features.\n",
    "\n",
    "#### ðŸ”¹ Output Layer\n",
    "- A final 3D convolution with softmax activation outputs a probability map for each class, per voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters):\n",
    "    x = layers.Conv3D(filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv3D(filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(x, filters):\n",
    "    f = conv_block(x, filters)\n",
    "    p = layers.MaxPooling3D((2, 2, 2))(f)\n",
    "    return f, p\n",
    "\n",
    "def decoder_block(x, skip, filters):\n",
    "    us = layers.UpSampling3D((2, 2, 2))(x)\n",
    "    concat = layers.Concatenate()([us, skip])\n",
    "    return conv_block(concat, filters)\n",
    "\n",
    "def build_3d_unet(input_shape=(128, 128, 128, 4), num_classes=4):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 32)\n",
    "    s2, p2 = encoder_block(p1, 64)\n",
    "    s3, p3 = encoder_block(p2, 128)\n",
    "    s4, p4 = encoder_block(p3, 256)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = conv_block(p4, 512)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b, s4, 256)\n",
    "    d2 = decoder_block(d1, s3, 128)\n",
    "    d3 = decoder_block(d2, s2, 64)\n",
    "    d4 = decoder_block(d3, s1, 32)\n",
    "\n",
    "    outputs = layers.Conv3D(num_classes, 1, activation='softmax')(d4)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30564160",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "### ðŸŽ¯ Dice Coefficient (Multiclass)\n",
    "\n",
    "The **Dice Coefficient** measures the overlap between predicted and ground truth segmentations. Itâ€™s especially useful in medical imaging tasks.\n",
    "\n",
    "\n",
    "#### ðŸ”¹ Per-Class Dice\n",
    "\n",
    "- Converts ground truth and prediction to binary masks for the target class.\n",
    "- Calculates:\n",
    "  \\[\n",
    "  \\text{Dice} = \\frac{2 \\cdot \\text{Intersection} + \\epsilon}{\\text{Sum of areas} + \\epsilon}\n",
    "  \\]\n",
    "- `epsilon` avoids division by zero.\n",
    "\n",
    "\n",
    "#### ðŸ”¹ Multiclass Dice\n",
    "\n",
    "- Averages the Dice score over all classes (4).\n",
    "- Provides a single score reflecting overall segmentation performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient_per_class(y_true, y_pred, class_index, smooth=1e-6):\n",
    "    \n",
    "    # Create binary masks for this class\n",
    "    y_true_c = tf.cast(tf.equal(y_true, class_index), tf.float32)\n",
    "    y_pred_c = tf.cast(tf.equal(tf.argmax(y_pred, axis=-1), class_index), tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_c * y_pred_c)\n",
    "    union = tf.reduce_sum(y_true_c) + tf.reduce_sum(y_pred_c)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice\n",
    "\n",
    "def multiclass_dice_coefficient(y_true, y_pred, num_classes=4, smooth=1e-6):\n",
    "    dice = 0\n",
    "    for i in range(num_classes):\n",
    "        dice += dice_coefficient_per_class(y_true, y_pred, i, smooth)\n",
    "    return dice / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_3d_unet()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[multiclass_dice_coefficient]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/content/BraTS2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\"\n",
    "val_dir = \"/content/BraTS2023/ASNR-MICCAI-BraTS2023-GLI-Challenge-ValidationData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a228cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all subject folders in train and val directories\n",
    "train_subject_dirs = sorted(glob(os.path.join(train_dir, \"*\")))\n",
    "val_subject_dirs = sorted(glob(os.path.join(val_dir, \"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0820f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_generator(train_subject_dirs, batch_size=2)\n",
    "val_gen = data_generator(val_subject_dirs, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd193b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"/Outputs/best_model.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "csv_logger = CSVLogger('/Outputs/training_log.csv', append=False)\n",
    "\n",
    "steps_per_epoch = math.ceil(len(train_subject_dirs) / 2)\n",
    "validation_steps = math.ceil(len(val_subject_dirs) / 2)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',    # metric to monitor\n",
    "    factor=0.5,            # factor to reduce LR by, new_lr = lr * factor\n",
    "    patience=3,            # number of epochs with no improvement before reducing LR\n",
    "    verbose=1,\n",
    "    min_lr=1e-7            # lower bound on LR\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, checkpoint , csv_logger , reduce_lr]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
