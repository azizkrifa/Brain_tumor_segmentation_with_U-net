# ğŸ§  Brain Tumor Segmentation with U-Net

Accurate segmentation of brain tumors, especially `gliomas`, is vital for diagnosis and treatment planning. The `BraTS 2024` dataset provides multi-modal MRI scans with expert annotations for tumor subregions.This project uses a `U-Net model` to automatically segment brain tumors from the BraTS 2024 MRI data. U-Netâ€™s architecture effectively captures tumor features to produce precise tumor masks, aiding clinical analysis.

-----

## 1. ğŸ“ Dataset

 ### 1.1 MRI Modalities and Segmentation Labels

  - The BraTS 2023 dataset includes `four MRI modalities` for each patient, providing complementary information to better identify tumor regions:
    
    - **T1-weighted (T1):** Provides detailed `anatomical structure` of the brain.  
    - **T1-contrast enhanced (T1c):** Highlights areas with a disrupted blood-brain barrier, such as `enhancing tumors`.  
    - **T2-weighted (T2w):** Useful for visualizing `edema` and `tumor boundaries`.  
    - **T2f or FLAIR (Fluid-Attenuated Inversion Recovery):** Suppresses fluid signals, making `edema` and `lesions` more visible.

<table align="center" >
  <tr>
    <td colspan="2" align="center">
      <h3>MRI Tests on the Same Patient X</h3>
    </td>
  </tr>

  <tr>
    <td align="center"><b>T1n</b></td>
    <td align="center"><b>T1c</b></td>
  </tr>
  <tr>
    <td><img width="600" height="3558" src="https://github.com/user-attachments/assets/93bd3548-af49-47c6-b8cf-881b033030dc" width="400"/></td>
    <td><img width="600" height="3558" src="https://github.com/user-attachments/assets/838a8056-48f7-4d56-b3d6-593fe11362ad" width="400"/></td>
  </tr>
  <tr>
    <td align="center"><b>T2f</b></td>
    <td align="center"><b>T2w</b></td>
  </tr>
  <tr>
    <td><img width="600" height="3558" src="https://github.com/user-attachments/assets/4b4f825a-c735-4de1-8dd4-3efb4b1d7ebb" width="400"/></td>
    <td><img width="600" height="3558" src="https://github.com/user-attachments/assets/57224994-f811-4102-bfa2-804cb9892012" width="400"/></td>
  </tr>
</table>

  - Along with these MRI scans, **segmentation masks (seg)** are provided. These masks label each voxel `(3D pixel)` as one of the following classes:
    
    - **0:** Background (non-tumor tissue)  
    - **1:** Necrotic and non-enhancing tumor core  
    - **2:** Peritumoral edema  
    - **3:** Enhancing tumor
    
    This multi-modal data enables the model to learn robust features across different tissue contrasts to accurately segment the tumor subregions.

  ### 1.2 Patient MRI Scan and Segmentation Dimensions

  - Each patient's MRI scan in the BraTS 2023 dataset has a fixed 3D volume size of **(240, 240, 155)**, representing:
    
    - **240 Ã— 240** pixels per slice (height Ã— width), capturing the spatial resolution of each 2D MRI slice.
    - **155** slices in the axial direction, representing the depth or number of cross-sectional images stacked to form the full 3D volume.
    
  - This 3D shape allows us to analyze the brainâ€™s structure slice-by-slice while preserving the volumetric context needed for accurate tumor segmentation.
    
    Below is an example visualization of the segmentation mask for patient X across some slices:
    
  <p align="center">
    <img width="7140" height="4734" alt="seg 02415" src="https://github.com/user-attachments/assets/cc047f52-11ce-4239-851d-d7cef5c5b628" />
  </p>

 ### 1.3 Dataset Structure|Ditribution
  **Note**: The test set was created by `randomly selecting 100 samples` from the original `training se`t to evaluate the model on unseen data while preserving label distribution.
  <p align="center">
    <img width="590" height="390" alt="Sans titre" src="https://github.com/user-attachments/assets/ba8972c5-f62b-445e-8d99-8bdbc90d072d" />
  </p>
  
```bash
  BraTS 2024 dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ BraTS-GLI-00001-000/
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00001-000_t2f.nii.gz
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00001-000_t1n.nii.gz
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00001-000_t1c.nii.gz
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00001-000_t2w.nii.gz
â”‚   â”‚   â””â”€â”€ BraTS-GLI-00001-000_seg.nii.gz
â”‚   â”œâ”€â”€ BraTS-GLI-00002-000/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...

â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ BraTS-GLI-00023-000/
â”‚   â”‚   â”œâ”€â”€ ... (same structure as train)
â”‚   â””â”€â”€ ...

â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ BraTS-GLI-00041-000/
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00041-000_t2f.nii.gz
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00041-000_t1n.nii.gz
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00041-000_t1c.nii.gz
â”‚   â”‚   â”œâ”€â”€ BraTS-GLI-00041-000_t2w.nii.gz
â”‚   â”‚   â””â”€â”€ BraTS-GLI-00041-000_seg.nii.gz
â”‚   â””â”€â”€ ...
```

  
  ### 1.4 Data Preprocessing
  
  To ensure consistency across all MRI scans and prepare the data for model training, we applied the following preprocessing steps to each subject:
  
  #### 1.4.1 Loading NIfTI Files  
  Each subject includes five volumes:  
  - T1 (T1n)  
  - T1 contrast-enhanced (T1c)  
  - T2-weighted (T2w)  
  - FLAIR (T2f)  
  - Segmentation mask  
  
  These files are loaded using the `nibabel` library, which reads `.nii.gz` medical imaging files and returns 3D NumPy arrays.
  
  #### 1.4.2 Normalization  
  For each modality (T1, T1c, T2, FLAIR), we apply `z-score normalization`:  

  <p align="center">
  <img width="299" height="70" alt="image" src="https://github.com/user-attachments/assets/be72d840-f8a7-47c5-ba98-9ba98edf2575" />
  </p>

  where $\mu$ and $\sigma$ are the mean and standard deviation of the image volume. This helps the model converge faster by standardizing intensity values.
  
  #### 1.4.3 Resizing Volumes  
   All modalities are resized from `240Ã—240Ã—155` to a fixed shape of `128Ã—128Ã—128` using linear interpolation (`order=1`). This standard shape ensures uniformity for training in 3D CNNs.
  
  #### 1.4.4 Resizing Segmentation Masks  
   Segmentation masks are resized using nearest-neighbor interpolation (`order=0`) to preserve discrete class labels (e.g., tumor regions). The output mask is cast to `uint8` type.
  
  #### 1.4.5 Multi-modal Stacking  
  The normalized and resized volumes from each modality are stacked along the last axis, resulting in a single 4D volume with shape `128Ã—128Ã—128Ã—4`. This format allows the model to learn from all four modalities simultaneously.





